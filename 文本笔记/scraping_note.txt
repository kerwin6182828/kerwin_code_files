
【项目一】
1.用scrapy框架爬取杭州租房网站（链家、Q房网、我爱我家）主城区的不同地段的房源信息（户型、面积、价格、标签、时间、小区、楼层、图片）
2.有时间还可以抓取进一步的数据（页面点进去）还有：浏览量、房屋朝向、地铁公交信息、经纪人电话、经纪人评分
有些房源没有地铁信息，但是“地铁距离”是数据分析的重要特征，缺失值需要自己去计算。调用高德地图的接口，获取距离最近地铁的距离，
3.由于爬取的是不同网站的信息，比较三个网站的价格，哪个比较低？
4.对数据进行清洗、特征提取、

对数据进行探索性可视化分析：
1.violin图
2.坐标图？（看看高德的接口难不难）想看一下不同价格区间的房源的坐标分布（希望能搞定）（主要是能不能获取到经纬度坐标）
更多画图技巧（查看kaggle大神。。。。）




5。用传统机器学习进行房价预测（线性回归、KNN、随机森林回归版本、xgboost不知道有没有回归版本）
6.用全连接的神经网络进行预测（看看分数会不会提升）

6.框架最好结合多线程并发执行、redis数据库、MongoDB， （**有时间再考虑广度优先和深度优先问题**）
7.考虑url池（队列）、代理ip池。（大不了买个几天）

用jupyter notebook的方式存在服务器上，方便面试时候展示。



相关系数分析：看看哪些房源的相关性最高。（即：如果我喜欢这类房子，那么我可能还会喜欢什么房子，我可以自己算出来）
(可以用无监督的降维分类方法：k-means或者PCA）
（不知道相关系数是怎么计算的？KNN寻找最近邻好像也可以，用的就是欧式距离，来找到最相近的？但是那个是做预测时候用的吧。。。不是分析用的）







【项目二】
1.对flickr进行图片爬取，分10类，并做训练和预测。（用TensorFlow+Keras 的Lenet结构的CNN）（在云服务器上跑）（多线程并行，这个简单）
（dog、cat、plane、
2.或者对英文字母和数字的图片进行爬取，然后做训练，以后自己可能还能用到？？
（字符分割、字符识别）——————（识别；最好是自己训练）（分割：可尝试多种分割方式然后喂给分类器，输出置信度，依旧置信度得到最后的预测结果）



自然语言处理：XX不做了
8.图片验证码不知道有没有？（这样也可以通过cnn学习一下，展现这方面技术）















每页30个房源

scrapy的请求url不是按照我写的顺序依次请求的，估计是异步的原因
        #每次yield一个item，都会经过pipeline，所以pipeline上的方法一定要多所有item都适用才行。。。

类方法的调用在类的实例化之前。（而且scrapy会在读取完所有模块组件（包括调用类方法这个马甲）之后，才开始open spider。。后面的实例方法才开始执行）
open spider之后最先执行的是spider里面的start_requests，然后该方法会yield一个Request对象发送请求，发送期间会经过middleware（先执行download中间件）
等requests发送后返回response时候先经过spider中间件处理，再回到spider模块的spider的parse实例方法。————接着返回item，或者继续生成下一轮Request实例。


scrapy框架的spider执行流程：

1.在spiders文件夹中逐个读取<spider>脚本（按目录中的脚本顺序依次读取）
2.读取middlewares.py脚本（只打印类外语句、类内方法外语句）————即：获取”类“的定义语句！
3.根据settings.py脚本实例化所有Downloader中间件  【先穿类方法的马甲，再执行__init__实例化】
4.根据settings.py脚本实例化所有Spider中间件    【同上】
5.当依次读完所有必要的脚本的类定义、类实例化后，正式开始open Spider！
6.首先执行的是spider中间件的spider_opened()方法将蜘蛛open，再是 process_start_requests()方法，
          然后才是<spider>脚本中的start_requests()方法，并yield一个实例化的Request（开始发送请求）
7.发送请求后途径各个Downloader中间件（小分优先原则），依次执行process_request()方法，修改请求参数
8.待对方服务器返回响应时，途径各个Spider中间件（小分原则），依次执行process_spider_input()方法
9.response最终返回到<spider>脚本的parse()方法，进行解析，提取需要的东西。此方法返回item对象，或者继续实例化新的Request请求。
10.item会被返回到pipeline脚本，该脚本可先对item对象的数据作清洗等，并另起一个MongoPipieline类进行存储操作！

注： 上述的中间件的更多方法没有具体列出来，但不影响大致的流程


重点：1.parse（）方法中的请求和解析逻辑 2.pipeline脚本中的数据清洗和存储 3.个性化定制中间件（cookies，ip池）  
高阶：1.selenium做中间件；splash异步并发ajax请求  2.通用化  3.redis分布式；web接口控制爬虫

疑惑：1. start_request 后的第一层页面访问都会变成301重定向。。。。（但是又重定向到原来网址。。。。）












