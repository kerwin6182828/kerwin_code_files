{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,mnistDemo\n",
    "\n",
    "#!usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Created on 2018年9月16日\n",
    "\n",
    "@author: yrh\n",
    "数据预处理\n",
    "'''\n",
    "\n",
    "'''\n",
    "=======================1、导入需要的包=======================\n",
    "\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "'''\n",
    "=======================2、卷积神经网络配置=======================\n",
    "\n",
    "'''\n",
    "# 卷积层 1\n",
    "#filter_size1 :   5 x 5 卷积核\n",
    "#num_filters1 :  共 16 个卷积核\n",
    "filter_size1 = 5\n",
    "num_filters1 = 16\n",
    "\n",
    "# 卷积层 2\n",
    "#filter_size2 : 5 x 5 卷积核\n",
    "#num_filters2 : 共 36 个卷积核\n",
    "filter_size2 = 5\n",
    "num_filters2 = 36\n",
    "\n",
    "\n",
    "img_size = 28  #图片大小\n",
    "img_flat = img_size * img_size   # \n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "\n",
    "# 全连接层\n",
    "#fc_size：128个神经元    \n",
    "fc_size = 128\n",
    "input_channels = 1\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "'''\n",
    "=======================3、载入数据 =========================\n",
    "\n",
    "TensorFlow在样例教程中已经做了下载并导入\n",
    "     MNIST数字手写体识别数据集的实现，可以直接使用\n",
    "     \n",
    "以下代码会将MNIST数据集下载到data/MNIST2目录下，\n",
    "    将标签保存为one-hot编码\n",
    "load_datas(filename)    \n",
    "========================================================\n",
    "'''\n",
    "def loadData(filename):\n",
    "    mnist = input_data.read_data_sets(filename, one_hot=False)\n",
    "    print(dir(mnist.train))\n",
    "    print(len(mnist.validation.images))\n",
    "    print(len(mnist.validation.labels))\n",
    "    print(np.array(mnist.train.images[0]))\n",
    "    print(np.array(mnist.train.images[0]).shape)\n",
    "    print(\"----------------------------------0------------------------------\")\n",
    "    print(\"----------------------------------0------------------------------\")\n",
    "    \n",
    "    print(np.array(mnist.train.images))\n",
    "    print(np.array(mnist.train.images).shape)\n",
    "    print(np.array(mnist.train.labels))\n",
    "    print(np.array(mnist.train.labels).shape)\n",
    "    print(\"----------------------------------1------------------------------\")\n",
    "    print(\"----------------------------------1------------------------------\")\n",
    "    \n",
    "    print(np.array(mnist.validation.labels))\n",
    "    print(np.array(mnist.validation.labels).shape)\n",
    "    print(\"----------------------------------2------------------------------\")\n",
    "    print(\"----------------------------------2------------------------------\")\n",
    "    \n",
    "    print(np.array(mnist.test.images))\n",
    "    print(np.array(mnist.test.images).shape)\n",
    "    print(np.array(mnist.test.labels))\n",
    "    print(np.array(mnist.test.labels).shape)\n",
    "    print(\"----------------------------------3------------------------------\")\n",
    "    print(\"----------------------------------3------------------------------\")\n",
    "    \n",
    "    print(mnist.validation.images[:8])\n",
    "    print(mnist.validation.labels[:8])\n",
    "    \n",
    "    \n",
    "#     mnist.train.cls = np.argmax(mnist.train.labels, axis=1)\n",
    "#     mnist.test.cls = np.argmax(mnist.test.labels, axis=1)\n",
    "#     mnist.validation.cls = np.argmax(mnist.validation.labels, axis=1)\n",
    "    return mnist\n",
    "\n",
    "    \n",
    "mnist = loadData(\"dataset/\") \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "MNIST数据集总共有70000张手写数字图片，\n",
    "数据集被分为训练集、测试集和验证集三部分\n",
    "Size of: \n",
    "-Training-set: 55000 \n",
    "-Test-set: 10000 \n",
    "-Validation-set: 5000\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "'''\n",
    "=======================4、数据维度=========================\n",
    "'''\n",
    "# 图片大小 img_size\n",
    "\n",
    " \n",
    "# 图片扁平化  img_size_flat\n",
    "\n",
    " \n",
    "# 图像维度  img_shape\n",
    "\n",
    " \n",
    "#颜色通道  num_channels \n",
    "\n",
    " \n",
    "#标签类别  num_classes\n",
    "\n",
    " \n",
    "'''\n",
    "=======================图片数据可视化=======================\n",
    "在3x3的栅格中显示9张图像\n",
    "plot_images(images, cls_true, cls_pred=None)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "# --------------代码测试-----------\n",
    "# 测试数据\n",
    "\n",
    "# 测试数据标签\n",
    "\n",
    "# 对测试数据和标签进行可视化\n",
    "\n",
    "\n",
    "def plot_images(imgs, cls, pred=None):\n",
    "    assert len(imgs) == len(cls) == 9\n",
    "    figure, axes = plt.subplots(3, 3)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(imgs[i].reshape(img_shape), cmap=\"binary\")\n",
    "        labl = \"\"\n",
    "        if pred is None:\n",
    "            lbl = \"cls:{0}\".format(cls[i])\n",
    "        else:\n",
    "            lbl = \"cls:{0}\".format(cls[i], pred[i])\n",
    "        ax.set_xlabel(lbl)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.show()\n",
    "    \n",
    "plot_images(mnist.train.images[:9], mnist.train.labels[:9])\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2,tf_graph\n",
    "\n",
    "#!usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Created on 2018年9月16日\n",
    "\n",
    "@author: yrh\n",
    "'''\n",
    "\n",
    "#导入\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "=========================1、创建变量=======================\n",
    "'''\n",
    "\n",
    "'''\n",
    "权重参数初始化\n",
    "零均值，0.01标准差正态分布的随机数\n",
    "new_weights\n",
    "'''\n",
    "\n",
    "# 这里的权重，应该就是卷积核上每个数值\n",
    "def new_weights(fileshape):\n",
    "    return tf.Variable(tf.random_normal(shape=fileshape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "偏置项初始化\n",
    "常量值\n",
    "new_biases\n",
    "'''\n",
    "def new_biases(num_filters):\n",
    "    return tf.constant(value=0.5, shape=[num_filters])\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "=========================2、创建卷积层=======================\n",
    "'''\n",
    "\n",
    "'''\n",
    "卷积层\n",
    "def new_conv_layer\n",
    "args：\n",
    "    前一层输入数据.\n",
    "    前一层通道数\n",
    "    卷积核尺寸\n",
    "    卷积核数目\n",
    "    使用 2x2 max-pooling.\n",
    "    \n",
    "return:结果层和权重\n",
    "\n",
    "'''\n",
    "\n",
    "def new_conv_layer(input_d, kernel_size, kernel_num, channel_num, use_pool=True):\n",
    "    # 实现：1.卷积操作   2.池化操作   3.激化函数操作    （全部都是tf的内置函数实现的）\n",
    "    \n",
    "    # ①宽、②高， ③颜色通道，④每一层中卷积核的块数， 卷积核的 \n",
    "\n",
    "    kernel_total_shape = [kernel_size, kernel_size, channel_num, kernel_num]\n",
    "    weights = new_weights(kernel_total_shape)\n",
    "    bias = new_biases(kernel_num)\n",
    "    \n",
    "    # 这里的filter就是所有卷积核  （即：一个4维的数组） ——将其作为TF卷积操作的参数即可\n",
    "    # 这里的strides 是按照 [卷积核数量，宽， 高 ，颜色通道]  的维度顺序来定义的 （注：与前面的weights不同）\n",
    "\t    # 这4个维度的顺序应该很好理解（先分页， 然后是行列的像素， 然后每个像素中包含三个通道）\n",
    "    layer = tf.nn.conv2d(input=input_d, filter=weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    layer += bias\n",
    "    \n",
    "    if use_pool:\n",
    "        # 因为池化是不能重叠的， 所以宽高的步长都是2\n",
    "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    layer = tf.nn.relu(layer)\n",
    "    # layer用于向后传递， weights用于传递到最后层后进行反向传播计算时候，需要不停修改的东西（这里的weights就是卷积核上的数值）\n",
    "    return layer, weights\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def new_con_layer(input_d, input_shape, filter_size, filter_num, use_pool=True):\n",
    "#     #宽、高、颜色通道、数量 \n",
    "#     shape = [filter_size, filter_size, input_shape, filter_num]\n",
    "#     weights = new_weights(shape)\n",
    "#     bias = new_biases(filter_num)\n",
    "    \n",
    "#     # 上一层输入的数据； 每层数据的每个权重参数； 移动的步长（对应shape上的4个维度）；填充类型（等长卷积）\n",
    "#     # 返回卷积层对象(经过卷积核扫描之后计算产生的值的那个结果层)\n",
    "#     # 在input_d就是上层的结果，也就是这一层的输入，也就是被卷积的数据\n",
    "#     layer = tf.nn.conv2d(input=input_d, filter=weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "#     layer += bias  # 直接加上偏置\n",
    "    \n",
    "#     if use_pool:\n",
    "#         layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "#     layer = tf.nn.relu(layer) # 添加relu激活函数（一般在隐藏层中使用， 输出层一般用sigmoid或者softmax）\n",
    "#     return layer, weights\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 卷积核权重的形状，由TensorFlow API决定\n",
    "    \n",
    "\n",
    "    # 根据给定形状创建权重\n",
    "    \n",
    "\n",
    "    # 创建新的偏置，每个卷积核一个偏置\n",
    "    \n",
    "\n",
    "    # 1、创建卷积层。注意stride全设置为1。\n",
    "    # 第1个和第4个必须是1，因为第1个是图像的数目，第4个是图像的通道。\n",
    "    # 第2和第3指定和左右、上下的步长。\n",
    "    # padding设置为'SAME' 意味着给图像补零，以保证前后像素相同。\n",
    "   \n",
    "\n",
    "    # 2、给卷积层的输出添加一个偏置，每个卷积通道一个偏置值\n",
    "   \n",
    "\n",
    "    # 3、是否使用pooling\n",
    "    \n",
    "        # 这是 2x2 max-pooling, 表明使用 2x2 的窗口，选择每一窗口的最大值作为该窗口的像素，\n",
    "        # 然后移动2格到下一窗口。\n",
    "        \n",
    "\n",
    "    # 4、 Linear Unit (ReLU).\n",
    "    # 对每个输入像素x，计算 max(x, 0)，把负数的像素值变为0.\n",
    "    # 这一步为原输出添加了一定的非线性特性，允许我们学习更加复杂的函数。\n",
    "   \n",
    "\n",
    "    # 注意 relu 通常在pooling前执行，但是由于 relu(max_pool(x)) == max_pool(relu(x))，\n",
    "    # 我们可以通过先max_pooling再relu省去75%的计算。\n",
    "\n",
    "    # 返回结果层和权重，结果层用于下一层输入，权重用于显式输出\n",
    "    \n",
    "   \n",
    "\n",
    "'''\n",
    "展平操作\n",
    "flatten_layer\n",
    "\n",
    "args： layer——上一层输入\n",
    "return：展平层、特征值维度\n",
    "'''\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    # 将取出上面卷积后传进来的layer的【宽、高、颜色通道】的维度  （更正：这里的通道不止是颜色通道，可以有更高的值）\n",
    "    layer_shape = layer.get_shape()\n",
    "    # 把上面取得的三维扁平化为一维（就是类似于np的flatten）\n",
    "    fcs = layer_shape[1:4].num_elements()\n",
    "    # 再加上【数量】这个维度， 变成一个两维的layer \n",
    "    #（第0个维度：表示有几张照片， 后面3个维度表示：已经将一张的特征提取出来，并仅用一个一维向量表示）\n",
    "    layer = tf.reshape(layer, shape=[-1, fcs])\n",
    "    \n",
    "    return layer, fcs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 获取输入层的形状，\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "    \n",
    "\n",
    "    # 特征数量: img_height * img_width * num_channels\n",
    "    # 可以使用TensorFlow内建操作计算.\n",
    "   \n",
    "\n",
    "    # 将形状重塑为 [num_images, num_features].\n",
    "    # 注意只设定了第二个维度的尺寸为num_filters，第一个维度为-1，保证第一个维度num_images不变\n",
    "    # 展平后的层的形状为:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "   \n",
    "\n",
    "    #返回展平层、特征值维度\n",
    "\n",
    "'''\n",
    "全连接层\n",
    "new_fc_layer\n",
    "args：\n",
    "    前一层输入\n",
    "    前一层输入维度\n",
    "    输出维度\n",
    "    是否使用relu\n",
    "    \n",
    "return：全连接层\n",
    "\n",
    "'''\n",
    "# 这里的input_d是指上一层经过扁平化后的一维向量??  (是一维吗？？？？上面返回的明明还有一个【数量】维度吗？)\n",
    "\n",
    "# 全连接层还没有理解\n",
    "def new_fc_layer(input_d, input_shape, out_shape, use_relu=True):\n",
    "    \n",
    "    weights = new_weights(fileshape=[input_shape, out_shape])\n",
    "    bias = new_biases(out_shape)\n",
    "    layer = tf.matmul(input_d, weights) + bias\n",
    "    \n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "    # 权重和偏置\n",
    "    \n",
    "\n",
    "    # 计算 y = wx + b\n",
    "   \n",
    "\n",
    "    # 是否使用RELU\n",
    "    \n",
    "\n",
    "    #返回层\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_4:0\", shape=(1, 14, 14, 16), dtype=float32)\n",
      "Tensor(\"Relu_5:0\", shape=(1, 7, 7, 36), dtype=float32)\n",
      "Tensor(\"Reshape_8:0\", shape=(1, 1764), dtype=float32)\n",
      "Tensor(\"Relu_6:0\", shape=(1, 128), dtype=float32)\n",
      "Tensor(\"add_7:0\", shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 3,placeholder\n",
    "\n",
    "#!usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Created on 2018年7月29日\n",
    "\n",
    "@author: yrh\n",
    "'''\n",
    "\n",
    "\n",
    "# 这些占位符相当于是tensorflow模型中， 方便用户更改的输入参数。（可以方便更改输入的训练样本等）\n",
    "\n",
    "'''\n",
    "=====================1、Placeholder占位符==========\n",
    "'''\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_flat], name='x')          # 原始输入（图像的一维向量， 像是每个像素点展开后的样子）\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, input_channels])                # 转换为2维图像（28*28的二维矩阵）\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, output_size], name='y_true')  # 原始输出  (即：一行10列的二维矩阵)\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)                  # 转换为真实类别，与之前的使用placeholder不同 \n",
    "# （y_true是独热编码后的，而y_true_cls是取出独热编码中概率最高的一个，还原回来，作为最终的预测值）\n",
    "\n",
    "\n",
    "'''\n",
    "卷积层 1\n",
    "'''\n",
    "\n",
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input_d=tf.reshape(mnist.train.images[0],(1, 28, 28, 1)),      # 输入图像\n",
    "                  channel_num=input_channels,         # 输入通道数\n",
    "                  kernel_size=filter_size1,          # 卷积核尺寸\n",
    "                  kernel_num = num_filters1,         # 卷积核数目\n",
    "                  use_pool=True)\n",
    "print(layer_conv1)\n",
    "#  千万注意： 这里返回的shape是（1*14*14*16）——→即：通道数从1，变成了这层的“核数”：16个！！！！！\n",
    "\n",
    "\n",
    "'''\n",
    "卷积层 2\n",
    "'''\n",
    "\n",
    "# 卷积之后往往把通道数叠加在一起了（具体怎么算我不知道，但返回的结果却是与通道数无关）\n",
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input_d=layer_conv1,      # 输入图像\n",
    "                  channel_num=num_filters1,         # 输入通道数   （注意： 此时的通道数不再是1了，而是上一层传下来的16了！！）\n",
    "                  kernel_size=filter_size2,          # 卷积核尺寸\n",
    "                  kernel_num = num_filters2,         # 卷积核数目\n",
    "                  use_pool=True)\n",
    "print(layer_conv2)\n",
    "\n",
    "\n",
    "'''\n",
    "展平层\n",
    "展平层将第二个卷积层展平为二维tensor。\n",
    "'''\n",
    "layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "print(layer_flat)\n",
    "\n",
    "'''\n",
    "全连接层 1\n",
    "'''\n",
    "# 每一层的神经元个数也是自定的\n",
    "# 每一个输入特征都要与每一个神经元相连，形成一个网状结构。（但计算机中，也可以用一个矩阵来表示网状）\n",
    "# 所以此处，传入输入特征和输出特征，可以直接形成\n",
    "layer_fc1 = new_fc_layer(input_d=layer_flat,           # 展平层输出\n",
    "                         input_shape=num_features,   # 输入特征维度    （展平层展开下来的维度）\n",
    "                         out_shape=fc_size,       # 输出特征维度    (此处为：该层中，输出的神经元个数) ——（开始就设置成常量128了）\n",
    "                         use_relu=True)\n",
    "print(layer_fc1)\n",
    "\n",
    "'''\n",
    "全连接层 2\n",
    "'''\n",
    "layer_fc2 = new_fc_layer(input_d=layer_fc1,           # 上一全连接层\n",
    "                         input_shape=fc_size,        # 输入特征维度\n",
    "                         out_shape=output_size,   # 输出类别数\n",
    "                         use_relu=False)\n",
    "print(layer_fc2)\n",
    "\n",
    "'''\n",
    "预测类别\n",
    "第二个全连接层估计输入的图像属于某一类别的程度，这个估计有些粗糙，需要添加一个softmax层归一化为概率表示。\n",
    "'''\n",
    "# 归一化后， 即为猜中某个值的概率（此时还在独热编码特征下，后面需要还原）（为什么归一化后的值就是概率， 可以参考逻辑回归的笔记）\n",
    "y_pred = tf.nn.softmax(layer_fc2)              # softmax归一化\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)         # 真实类别\n",
    "# 在独热编码的10个特征下，取出最大的索引， 而这个索引就是相对应的的最终预测值！！\n",
    "\n",
    "'''\n",
    "代价函数\n",
    "概率交叉熵\n",
    "'''\n",
    "# 定义损失函数的“策略”\n",
    "# 输入神经网络中最后层的输出结果， 再输入每个样本的真实值，即可得到cost函数的公式（再在后面的梯度下降算法中使用）\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "'''   \n",
    "优化方法\n",
    "'''\n",
    "# 优化器（即：梯度下降算法）\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "'''\n",
    "性能度量\n",
    "'''\n",
    "\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls) # 猜中为True， 猜错为False\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# 先用cast函数将布尔值转为“float”，再用reduce_mean求出均值，作为准确率\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6955.4004 ,   9742.07   ,   1745.8453 ,   1642.3376 ,\n",
       "        -13516.663  ,  -7704.8296 ,  -5219.955  ,   5891.5547 ,\n",
       "         -5192.6396 ,    840.25903]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64,) for Tensor 'y_true_2:0', which has shape '(?, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8066c1e6aa8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m '''\n\u001b[1;32m    315\u001b[0m \u001b[0;31m#10000轮次优化后的性能\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m print_test_accuracy(show_example_errors=True,\n\u001b[1;32m    318\u001b[0m                     show_confusion_matrix=True)\n",
      "\u001b[0;32m<ipython-input-14-8066c1e6aa8c>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(num_iterations)\u001b[0m\n\u001b[1;32m     77\u001b[0m                           y_true: y_true_batch}\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# 运行优化器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# 每100轮迭代输出状态\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    962\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64,) for Tensor 'y_true_2:0', which has shape '(?, 10)'"
     ]
    }
   ],
   "source": [
    "# 4,test\n",
    "\n",
    "#!usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Created on 2018年7月28日\n",
    "\n",
    "@author: yrh\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "'''\n",
    "==========================================\n",
    "        保存训练模型\n",
    "====================================================\n",
    "'''\n",
    "# 为了保存神经网络的变量，我们创建一个称为Saver-object的对象，\n",
    "# 它用来保存及恢复TensorFlow图的所有变量\n",
    "# 保存操作在后面的optimize()函数中完成\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "save_dir='checkpoints/'   # 保存路径\n",
    "if not os.path.exists(save_dir):  #如果路径不存在\n",
    "    os.makedirs(save_dir)   # 就创建它\n",
    "    \n",
    "save_path=os.path.join(save_dir,'best_validation')  #模型保存路径\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "==========================================\n",
    "        运行Placeholder占位符\n",
    "====================================================\n",
    "'''\n",
    "\n",
    "'''\n",
    "1、创建session\n",
    "'''\n",
    "session=tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "'''\n",
    "2、执行优化函数\n",
    "'''\n",
    "train_batch_size=64\n",
    "\n",
    "#当前迭代次数\n",
    "total_iterations=0\n",
    "\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    #初始化变量\n",
    "    global total_iterations\n",
    "    \n",
    "    # 最优验证准确率.\n",
    "    best_validation_accuracy = 0.0\n",
    "    \n",
    "    # 最后更新验证准确率时的迭代次数.\n",
    "    last_improvement = 0\n",
    "    \n",
    "    # 停止更新优化.\n",
    "    require_improvement = 1000\n",
    "    \n",
    "    \n",
    "    # 用来输出用时.\n",
    "    start_time = time.time()\n",
    "    for i in range(total_iterations,total_iterations+num_iterations):\n",
    "         # 获取一批数据，放入dict\n",
    "        x_batch, y_true_batch = mnist.train.next_batch(train_batch_size)\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                          y_true: y_true_batch}\n",
    "        # 运行优化器\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # 每100轮迭代输出状态\n",
    "        if i % 100 == 0:\n",
    "            # 计算训练集准确率.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            msg = \"迭代轮次: {0:>6}, 训练准确率: {1:>6.1%}\"\n",
    "            print(msg.format(i + 1, acc))\n",
    "            \n",
    "            acc_validation, _ = validation_accuracy()\n",
    "            if acc_validation>best_validation_accuracy:\n",
    "                best_validation_accuracy=acc_validation\n",
    "                last_improvement=total_iterations\n",
    "                 \n",
    "                saver.save(sess=session, save_path=save_path)\n",
    "\n",
    "               \n",
    "                improved_str = '*'\n",
    "            else:\n",
    "                \n",
    "                improved_str = ''\n",
    "            \n",
    "            \n",
    "            msg = \"Iter: {0:>6}, Train-Batch Accuracy: {1:>6.1%}, Validation Acc: {2:>6.1%} {3}\"\n",
    "\n",
    "            # 打印.\n",
    "            print(msg.format(i + 1, acc, acc_validation, improved_str))\n",
    "\n",
    "        # 如果超过1000次验证准确率没有更新.\n",
    "        if total_iterations - last_improvement > require_improvement:\n",
    "            print(\"No improvement found in a while, stopping optimization.\")\n",
    "\n",
    "            # 停止优化.\n",
    "            break\n",
    "            \n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # 输出用时.\n",
    "    print(\"用时: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "3、输出部分错误样例与混淆矩阵\n",
    "'''\n",
    "def plot_example_errors(cls_pred, correct):\n",
    "    # 计算错误情况\n",
    "    incorrect = (correct == False)\n",
    "    images = mnist.test.images[incorrect]\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "    cls_true = mnist.test.cls[incorrect]\n",
    "\n",
    "    # 随机挑选9个\n",
    "    indices = np.arange(len(images))\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[:9]\n",
    "\n",
    "    plot_images(images[indices], cls_true[indices], cls_pred[indices])\n",
    "\n",
    "def plot_confusion_matrix(cls_pred):\n",
    "    cls_true = mnist.test.cls  # 真实类别  \n",
    "\n",
    "    # 使用scikit-learn的confusion_matrix来计算混淆矩阵\n",
    "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
    "\n",
    "    # 打印混淆矩阵\n",
    "    print(cm)\n",
    "\n",
    "    # 将混淆矩阵输出为图像\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # 调整图像\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "4、测试\n",
    "'''\n",
    "\n",
    "# 将测试集分成更小的批次\n",
    "test_batch_size = 256\n",
    "\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=False):\n",
    "    # 测试集图像数量.\n",
    "    num_test = len(mnist.test.images)\n",
    "\n",
    "    # 为预测结果申请一个数组.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # 数据集的起始id为0\n",
    "    i = 0\n",
    "    while i < num_test:\n",
    "        # j为下一批次的截止id\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # 获取i，j之间的图像\n",
    "        images = mnist.test.images[i:j, :]\n",
    "\n",
    "        # 获取相应标签.\n",
    "        labels = mnist.test.labels[i:j, :]\n",
    "\n",
    "        # 创建feed_dict\n",
    "        feed_dict = {x: images,\n",
    "                    y_true: labels}\n",
    "\n",
    "        # 计算预测结果\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # 设定为下一批次起始值.\n",
    "        i = j\n",
    "\n",
    "    cls_true = mnist.test.cls\n",
    "    # 正确的分类\n",
    "    correct = (cls_true == cls_pred)\n",
    "    # 正确分类的数量\n",
    "    correct_sum = correct.sum()\n",
    "    # 分类准确率\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # 打印准确率.\n",
    "    msg = \"测试集准确率: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # 打印部分错误样例.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # 打印混淆矩阵.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)    \n",
    "        \n",
    "    return correct,cls_pred\n",
    " \n",
    "# 预测类别.\n",
    "batch_size = 256\n",
    "\n",
    "def predict_cls(images, labels, cls_true):\n",
    "    # Number of images.\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_images, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_images:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_images)\n",
    "\n",
    "        # Create a feed-dict with the images and labels\n",
    "        # between index i and j.\n",
    "        feed_dict = {x: images[i:j, :],\n",
    "                     y_true: labels[i:j, :]}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    return correct, cls_pred \n",
    "        \n",
    "# 计算测试集上的预测类别\n",
    "def predict_cls_test():\n",
    "    return predict_cls(images = mnist.test.images,\n",
    "                       labels = mnist.test.labels,\n",
    "                       cls_true = mnist.test.cls)\n",
    "\n",
    "#计算验证集上的预测类别\n",
    "def predict_cls_validation():\n",
    "    return predict_cls(images = mnist.validation.images,\n",
    "                       labels = mnist.validation.labels,\n",
    "                       cls_true = mnist.validation.cls)\n",
    "    \n",
    "    \n",
    "#分类准确率的帮助函数\n",
    "#这个函数计算了给定布尔数组的分类准确率，布尔数组表示每张图像是否被正确分类\n",
    "def cls_accuracy(correct):\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / len(correct)\n",
    "\n",
    "    return acc, correct_sum\n",
    "\n",
    "#计算验证集上的分类准确率\n",
    "def validation_accuracy():\n",
    "    # Get the array of booleans whether the classifications are correct\n",
    "    # for the validation-set.\n",
    "    # The function returns two values but we only need the first.\n",
    "    correct, _ = predict_cls_validation()\n",
    "    \n",
    "    # Calculate the classification accuracy and return it.\n",
    "    return cls_accuracy(correct)\n",
    "\n",
    "'''    \n",
    "#优化前的性能测试\n",
    "print_test_accuracy()\n",
    "\n",
    "#执行一轮优化后的性能\n",
    "optimize(num_iterations=1)\n",
    "print_test_accuracy()\n",
    "    \n",
    "#100轮优化后的性能\n",
    "optimize(num_iterations=99)\n",
    "print_test_accuracy()\n",
    "\n",
    "#1000轮优化后性能\n",
    "optimize(num_iterations=999)\n",
    "print_test_accuracy(show_example_errors=True)\n",
    "'''\n",
    "#10000轮次优化后的性能\n",
    "optimize(num_iterations=99)\n",
    "print_test_accuracy(show_example_errors=True,\n",
    "                    show_confusion_matrix=True)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "==========================================\n",
    "        权重和层的可视化\n",
    "====================================================\n",
    "'''\n",
    "#卷积权重可视化\n",
    "def plot_conv_weights(weights, input_channel=0):\n",
    "    # weights_conv1 or weights_conv2.\n",
    "\n",
    "    # 运行weights以获得权重\n",
    "    w = session.run(weights)\n",
    "\n",
    "    # 获取权重最小值最大值，这将用户纠正整个图像的颜色密集度，来进行对比\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "\n",
    "    # 卷积核\n",
    "    num_filters = w.shape[3]\n",
    "\n",
    "    # 需要输出的卷积核\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # 只输出有用的子图.\n",
    "        if i<num_filters:\n",
    "            # 获得第i个卷积核在特定输入通道上的权重\n",
    "            img = w[:, :, input_channel, i]\n",
    "\n",
    "            ax.imshow(img, vmin=w_min, vmax=w_max,\n",
    "                      interpolation='nearest', cmap='seismic')\n",
    "\n",
    "        # 移除坐标.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#卷积层输出可视化\n",
    "def plot_conv_layer(layer, image):\n",
    "    # layer_conv1 or layer_conv2.\n",
    "\n",
    "    # feed_dict只需要x，标签信息在此不需要.\n",
    "    feed_dict = {x: [image]}\n",
    "\n",
    "    # 获取该层的输出结果\n",
    "    values = session.run(layer, feed_dict=feed_dict)\n",
    "\n",
    "    # 卷积核\n",
    "    num_filters = values.shape[3]\n",
    "\n",
    "    # 每行需要输出的卷积核网格数\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # 只输出有用的子图.\n",
    "        if i<num_filters:\n",
    "            # 获取第i个卷积核的输出\n",
    "            img = values[0, :, :, i]\n",
    "\n",
    "            ax.imshow(img, interpolation='nearest', cmap='binary')\n",
    "\n",
    "        # 移除坐标.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#打印输入图像\n",
    "def plot_image(image):\n",
    "    plt.imshow(image.reshape(img_shape),\n",
    "              interpolation='nearest',\n",
    "              cmap='binary')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#打印第一张，第二张图像：\n",
    "image1 = mnist.test.images[0]\n",
    "plot_image(image1)\n",
    "\n",
    "image2 = mnist.test.images[13]\n",
    "plot_image(image2)\n",
    "\n",
    "#卷积层 1\n",
    "#权重\n",
    "plot_conv_weights(weights=weights_conv1)\n",
    "#输出\n",
    "plot_conv_layer(layer=layer_conv1, image=image1)\n",
    "plot_conv_layer(layer=layer_conv1, image=image2)\n",
    "\n",
    "#卷积层 2\n",
    "\n",
    "#第1个通道的权重\n",
    "plot_conv_weights(weights=weights_conv2, input_channel=0)\n",
    "#第2个通道的权重\n",
    "plot_conv_weights(weights=weights_conv2, input_channel=1)\n",
    "#images1输出\n",
    "plot_conv_layer(layer=layer_conv1, image=image2)\n",
    "#images2输出\n",
    "plot_conv_layer(layer=layer_conv1, image=image2)\n",
    "\n",
    "\n",
    "#关闭session\n",
    "session.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
